{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "d58AghKA4r49",
        "fGlzpySg4xbr",
        "PDZKOxrh-KCs",
        "jawlqKI15J7u",
        "xCbTYko95YKM",
        "W93I4nlfDrAY",
        "ymvn9xOQD-DJ",
        "7xn_-yOYEuzw",
        "kjvHrw-MFxpq",
        "L0YjqLNcF255",
        "sYb5Ou_sGjQS",
        "pd3XFnOdHDq2",
        "JzItTZp2Hudw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Notebook para selección del mejor algoritmo de Machine Learning**\n",
        "Con la finalidad de encontrar el algoritmo que mas se ajusta al set de datos propuesto y con el esquema de preselección de 6 algortimos que cumplían con unas características especificas, se seleccionaron estos algoritmos para realizar las pruebas:\n",
        "* Multi-Layer Perceptron (MLP)\n",
        "* Random Forest (RF)\n",
        "* Light Gradient Boosting Machine (LightGBM)\n",
        "* Extreme Gradient Boosting (XGB)\n",
        "* Support Vector Machine (SVM)\n",
        "* Isolation Forest (iForest)\n",
        "\n",
        "Se realizaron los ajustes de las librerías necesarias y se efectuó un entrenamiento asistido con los métodos ofrecidos por las librerías de cada algoritmo.\n",
        "\n",
        "### **Estructura del notebook:**\n",
        "1. Instalación de dependencias\n",
        "2. Cargue de archivos CSV (gps_valid.csv)\n",
        "3. Importe y carga de datos\n",
        "4. Función Haversine\n",
        "5. Función de generación datos spoofing\n",
        "6. Entrenamiento con MLP\n",
        "7. Entrenamiento con RF\n",
        "8. Entrenamiento con LightGBM\n",
        "9. Entrenamiento con RF con XGB\n",
        "10. Entrenamiento con iForest\n",
        "11. Entrenamiento con SVM\n",
        "\n"
      ],
      "metadata": {
        "id": "poZ_Y7IB1DSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Instalación de dependencias**"
      ],
      "metadata": {
        "id": "d58AghKA4r49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === INSTALAR DEPENDENCIAS ===\n",
        "!pip install --upgrade --force-reinstall numpy scipy scikit-learn pandas joblib xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J8yxDYWk1TR2",
        "outputId": "192b4868-667e-4d33-94b0-6db49ecd20a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting joblib\n",
            "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting xgboost\n",
            "  Downloading xgboost-3.0.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.26.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgboost-3.0.1-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (318.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, threadpoolctl, six, nvidia-nccl-cu12, numpy, joblib, scipy, python-dateutil, xgboost, scikit-learn, pandas\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.0\n",
            "    Uninstalling joblib-1.5.0:\n",
            "      Successfully uninstalled joblib-1.5.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.4\n",
            "    Uninstalling xgboost-2.1.4:\n",
            "      Successfully uninstalled xgboost-2.1.4\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.3\n",
            "    Uninstalling pandas-2.2.3:\n",
            "      Successfully uninstalled pandas-2.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nccl-cu12==2.21.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.26.5 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.5.0 numpy-2.2.6 nvidia-nccl-cu12-2.26.5 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 scikit-learn-1.6.1 scipy-1.15.3 six-1.17.0 threadpoolctl-3.6.0 tzdata-2025.2 xgboost-3.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "six"
                ]
              },
              "id": "0a4345ea8958461d80af29c04cf36883"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Cargue de archivos CSV (gps_valid.csv)**"
      ],
      "metadata": {
        "id": "fGlzpySg4xbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === SUBIR CSV ===\n",
        "from google.colab import files\n",
        "\n",
        "print(\"🔃 Sube aquí tu archivo gps_valid.csv (datos íntegros sin spoofing)\")\n",
        "uploaded = files.upload()  # Selecciona gps_valid.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "tEwdZ-Jy5CFj",
        "outputId": "e0a236e4-8e15-4390-9844-26dc9785b6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔃 Sube aquí tu archivo gps_valid.csv (datos íntegros sin spoofing)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cd2b26b8-9f01-4f82-9b79-d00aee0266dd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cd2b26b8-9f01-4f82-9b79-d00aee0266dd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving gps_valid.csv to gps_valid.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Importe y carga de datos**"
      ],
      "metadata": {
        "id": "PDZKOxrh-KCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === IMPORTS Y CARGA DE DATOS ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from math import radians, sin, cos, sqrt, atan2, degrees\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from google.colab import files as gfiles\n",
        "\n",
        "# Nombre del CSV subido\n",
        "CSV_FILE = \"gps_valid.csv\"\n",
        "\n",
        "# Cargar y parsear\n",
        "df = pd.read_csv(CSV_FILE, parse_dates=[\"UTC\"])\n",
        "# Separar Lat/Lon\n",
        "df[[\"Lat\",\"Lon\"]] = df[\"Position\"].str.split(\",\", expand=True).astype(float)\n",
        "df = df.sort_values(\"UTC\").reset_index(drop=True)\n",
        "\n",
        "print(f\"✅ Datos cargados: {len(df)} filas\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQPBCTpi5Q03",
        "outputId": "34a1fb08-bdf7-40a2-cff1-eed00567b092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Datos cargados: 7510 filas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Función Haversine**"
      ],
      "metadata": {
        "id": "jawlqKI15J7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === CÁLCULO DE FEATURES DE CONTINUIDAD ===\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371000  # radio terrestre en metros\n",
        "    φ1, φ2 = radians(lat1), radians(lat2)\n",
        "    Δφ = radians(lat2 - lat1)\n",
        "    Δλ = radians(lon2 - lon1)\n",
        "    a = sin(Δφ/2)**2 + cos(φ1)*cos(φ2)*sin(Δλ/2)**2\n",
        "    d = 2 * R * atan2(sqrt(a), sqrt(1-a))\n",
        "    y = sin(Δλ) * cos(φ2)\n",
        "    x = cos(φ1)*sin(φ2) - sin(φ1)*cos(φ2)*cos(Δλ)\n",
        "    θ = (degrees(atan2(y, x)) + 360) % 360\n",
        "    return d, θ\n",
        "\n",
        "def compute_continuity_features(df):\n",
        "    dists, bears = [], []\n",
        "    for i in range(1, len(df)):\n",
        "        d, b = haversine(\n",
        "            df.loc[i-1,\"Lat\"], df.loc[i-1,\"Lon\"],\n",
        "            df.loc[i  ,\"Lat\"], df.loc[i  ,\"Lon\"]\n",
        "        )\n",
        "        dists.append(d)\n",
        "        bears.append(b)\n",
        "    return pd.DataFrame({\n",
        "        \"dist\":    np.array(dists, dtype=np.float32),\n",
        "        \"bearing\": np.array(bears, dtype=np.float32)\n",
        "    })\n",
        "\n",
        "cont_df = compute_continuity_features(df)\n",
        "y_clean = np.ones(len(cont_df), dtype=int)\n",
        "\n",
        "print(f\"✅ {len(cont_df)} muestras de continuidad calculadas\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSl82JSJ5WKl",
        "outputId": "2204188a-d3f5-4d75-dafb-d72091f7b3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 7509 muestras de continuidad calculadas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Función de generación datos spoofing**"
      ],
      "metadata": {
        "id": "xCbTYko95YKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === GENERACIÓN DE ANOMALÍAS SINTÉTICAS ===\n",
        "SEED = 42\n",
        "ANOMALY_RATIO = 0.2\n",
        "\n",
        "np.random.seed(SEED)\n",
        "n = len(cont_df)\n",
        "n_anom = int(n * ANOMALY_RATIO)\n",
        "# Seleccion de indices del dataset original\n",
        "idx = np.random.choice(n, n_anom, replace=False)\n",
        "\n",
        "# Creacion de un arreglo con valores de spoofing generados\n",
        "cont_anom_features = []\n",
        "for j in idx:\n",
        "    # Toma de datos del dataset original\n",
        "    d_orig, b_orig = cont_df.loc[j, \"dist\"], cont_df.loc[j, \"bearing\"]\n",
        "    lat0, lon0 = df.loc[j, \"Lat\"], df.loc[j, \"Lon\"] # Uso de la funcion Haversine para carcualar valores de latitud y longitud\n",
        "\n",
        "    # Generacion de parametros aleatorios fuera de ruta entre 5 y 50 kilometros (teniendo en cuenta que el radio esta en kilometros)\n",
        "    jump_d = np.random.uniform(5_000, 50_000)\n",
        "    jump_b = np.random.uniform(0, 360)\n",
        "\n",
        "    # El calculo asume que se tiene un salto a una nueva posicion, simulando un spoofing\n",
        "    # La anomalia estaria en el punto generado (df.loc[j])\n",
        "    φ = radians(lat0)\n",
        "    Δφ = (jump_d * cos(radians(jump_b))) / 6371000\n",
        "    Δλ = (jump_d * sin(radians(jump_b))) / (6371000 * cos(φ))\n",
        "    lat1 = lat0 + degrees(Δφ)\n",
        "    lon1 = lon0 + degrees(Δλ)\n",
        "\n",
        "    '''\n",
        "    Calcular las características con la funcion  Haversine para el punto anómalo en relación con el punto anterior, asumiendo que el punto anterior es el que está en el índice j-1 en el DataFrame original (df).\n",
        "    Se usara la interpretación de que el salto modifica la ubicación del punto j y que la anomalía se detecta en la característica de continuidad entre j-1 y el j modificado.\n",
        "    Esto significa que el cálculo con la funcion Haversine debe realizarse entre `df.loc[j-1]` y la nueva ubicación `(lat1, lon1)`.\n",
        "    Las características `dist` y `bearing` en el índice `i` dentro de `cont_df` corresponden a la continuidad entre `df.loc[i-1]` y `df.loc[i]`.\n",
        "    Si queremos introducir una anomalía en el índice `j` de `cont_df`, deberíamos modificar las características de continuidad entre `df.loc[j-1]` y una nueva ubicación para `df.loc[j]`.\n",
        "\n",
        "    Las características anómalas deberían calcularse entre `df.loc[j]` y una nueva ubicación para `df.loc[j+1]`.\n",
        "    El código original usaba `df.loc[j]` como punto de inicio del salto y aplicaba el salto para obtener un nuevo punto (lat1, lon1).\n",
        "    Luego calculaba Haversine entre `df.loc[j]` y este nuevo punto. Este cálculo no reemplaza directamente el valor original en `cont_df[j]` (que es entre `df[j]` y `df[j+1]`).\n",
        "\n",
        "    Supongamos entonces que el objetivo es crear puntos de spoofing. Un paso anómalo en el índice `j` de `cont_df` significa que la transición desde `df.loc[j]` hacia `df.loc[j+1]` es anómala.\n",
        "    Deberíamos tomar `df.loc[j]` como punto de partida y aplicar un salto para encontrar una nueva ubicación para el siguiente punto (que sería el punto anómalo `df.loc[j+1]`).\n",
        "    '''\n",
        "\n",
        "    lat_start, lon_start = df.loc[j, \"Lat\"], df.loc[j, \"Lon\"]\n",
        "\n",
        "    φ_start = radians(lat_start)\n",
        "    Δφ = (jump_d * cos(radians(jump_b))) / 6371000\n",
        "    Δλ = (jump_d * sin(radians(jump_b))) / (6371000 * cos(φ_start))\n",
        "    lat_anom_next = lat_start + degrees(Δφ)\n",
        "    lon_anom_next = lon_start + degrees(Δλ)\n",
        "\n",
        "    # Calcular las características de Haversine para este paso anómalo (entre df.loc[j] y la nueva ubicación para df.loc[j+1])\n",
        "    d_anom, b_anom = haversine(lat_start, lon_start, lat_anom_next, lon_anom_next)\n",
        "    cont_anom_features.append([d_anom, b_anom])\n",
        "\n",
        "# Se convierte el listado de anomalias en un numpy array\n",
        "cont_anom_features = np.array(cont_anom_features, dtype=np.float32)\n",
        "\n",
        "y_anom = np.zeros(n_anom, dtype=int)\n",
        "\n",
        "# Se combinan los datos del dataset entre integros 1 y anomalos o spoofing 0\n",
        "X = np.vstack([cont_df.values, cont_anom_features])\n",
        "y = np.concatenate([y_clean, y_anom])\n",
        "\n",
        "print(f\"✅ Dataset combinado: {X.shape[0]} muestras ({len(y_clean)} íntegros + {n_anom} anómalos)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie6LdTxF5b20",
        "outputId": "2e6146bf-5c55-45d5-a5bd-7945cd33620e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset combinado: 9010 muestras (7509 íntegros + 1501 anómalos)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Entrenamiento con MLP**"
      ],
      "metadata": {
        "id": "W93I4nlfDrAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENTRENAMIENTO MLP ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(64, 32),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=200,\n",
        "    random_state=SEED,\n",
        "    verbose=True\n",
        ")\n",
        "mlp.fit(X_train, y_train)\n",
        "print(\"✅ MLP entrenado correctamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuufZEwn5kcK",
        "outputId": "f75a8497-3b15-4bc4-ca0a-b23e632990f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 5.57524007\n",
            "Iteration 2, loss = 2.32617669\n",
            "Iteration 3, loss = 1.64645100\n",
            "Iteration 4, loss = 2.05241832\n",
            "Iteration 5, loss = 1.96156036\n",
            "Iteration 6, loss = 2.08208372\n",
            "Iteration 7, loss = 1.88712903\n",
            "Iteration 8, loss = 1.53279043\n",
            "Iteration 9, loss = 1.47457927\n",
            "Iteration 10, loss = 1.77926970\n",
            "Iteration 11, loss = 1.96757365\n",
            "Iteration 12, loss = 1.93428265\n",
            "Iteration 13, loss = 1.69848470\n",
            "Iteration 14, loss = 1.23135989\n",
            "Iteration 15, loss = 1.70129981\n",
            "Iteration 16, loss = 2.41347051\n",
            "Iteration 17, loss = 2.60123147\n",
            "Iteration 18, loss = 1.73810149\n",
            "Iteration 19, loss = 2.97231880\n",
            "Iteration 20, loss = 1.59230060\n",
            "Iteration 21, loss = 1.38349140\n",
            "Iteration 22, loss = 2.17824599\n",
            "Iteration 23, loss = 1.94167175\n",
            "Iteration 24, loss = 1.87374947\n",
            "Iteration 25, loss = 2.41018855\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "✅ MLP entrenado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.1 Evaluación y generación modelo**"
      ],
      "metadata": {
        "id": "ymvn9xOQD-DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUACIÓN Y GUARDADO DEL MLP ===\n",
        "y_pred = mlp.predict(X_test)\n",
        "print(\"=== Reporte de clasificación MLP ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC AUC (MLP):\", roc_auc_score(y_test, mlp.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# Serializar y descargar el modelo\n",
        "joblib.dump(mlp, \"mlp_continuity_model.pkl\")\n",
        "print(\"✅ MLP serializado en 'mlp_continuity_model.pkl'\")\n",
        "gfiles.download(\"mlp_continuity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "2Tmv07jO54Uo",
        "outputId": "3a5643fa-c0ea-43ee-9552-04a3b13fd953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Reporte de clasificación MLP ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6489    0.8133    0.7219       300\n",
            "           1     0.9607    0.9121    0.9358      1502\n",
            "\n",
            "    accuracy                         0.8957      1802\n",
            "   macro avg     0.8048    0.8627    0.8288      1802\n",
            "weighted avg     0.9088    0.8957    0.9002      1802\n",
            "\n",
            "ROC AUC (MLP): 0.9358710608078118\n",
            "✅ MLP serializado en 'mlp_continuity_model.pkl'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_02039291-6428-4b4c-921e-4fe46e436c6f\", \"mlp_continuity_model.pkl\", 45553)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Entrenamiento con RF**"
      ],
      "metadata": {
        "id": "7xn_-yOYEuzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENTRENAMIENTO DEL RANDOM FOREST ===\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# División estratificada (usa X, y ya definidos en celdas anteriores)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "# Configuración y entrenamiento del Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "print(\"✅ Random Forest entrenado correctamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33xbIOMR7hOO",
        "outputId": "be3868f5-b4b2-4101-dadf-f8dfde2e4b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Random Forest entrenado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.1 Evaluación y generación modelo**"
      ],
      "metadata": {
        "id": "kjvHrw-MFxpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUACIÓN Y GUARDADO DEL RANDOM FOREST ===\n",
        "# Predicciones y métricas\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"=== Reporte de clasificación RF ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC AUC (RF):\", roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# Serializar y descargar el modelo\n",
        "joblib.dump(rf, \"rf_continuity_model.pkl\")\n",
        "print(\"✅ RF serializado en 'rf_continuity_model.pkl'\")\n",
        "gfiles.download(\"rf_continuity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "cSgMqJq77joF",
        "outputId": "56b6c58e-f293-4a63-afa4-e473e2561c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Reporte de clasificación RF ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9601    0.9633    0.9617       300\n",
            "           1     0.9927    0.9920    0.9923      1502\n",
            "\n",
            "    accuracy                         0.9872      1802\n",
            "   macro avg     0.9764    0.9777    0.9770      1802\n",
            "weighted avg     0.9873    0.9872    0.9872      1802\n",
            "\n",
            "ROC AUC (RF): 0.9963371060807812\n",
            "✅ RF serializado en 'rf_continuity_model.pkl'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_63713a68-fa09-4903-94ea-1da5907f529e\", \"rf_continuity_model.pkl\", 2778889)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Entrenamiento con LightGBM**"
      ],
      "metadata": {
        "id": "L0YjqLNcF255"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENTRENAR con LightGBM ===\n",
        "!pip install lightgbm\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Usamos X, y, SEED ya definidos en las celdas anteriores\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "# Configuración y entrenamiento del modelo LightGBM\n",
        "lgbm = LGBMClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.1,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "lgbm.fit(X_train, y_train)\n",
        "print(\"✅ LightGBM entrenado correctamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wTckVTK7rSL",
        "outputId": "834804da-248e-4657-9201-72c12f0113a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
            "[LightGBM] [Info] Number of positive: 6007, number of negative: 1201\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000783 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 510\n",
            "[LightGBM] [Info] Number of data points in the train set: 7208, number of used features: 2\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.833380 -> initscore=1.609771\n",
            "[LightGBM] [Info] Start training from score 1.609771\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "✅ LightGBM entrenado correctamente\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.1 Evaluación y generación modelo**"
      ],
      "metadata": {
        "id": "LYwprR5_GMsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUACIÓN Y GUARDADO LightGBM ===\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import joblib\n",
        "from google.colab import files as gfiles\n",
        "\n",
        "# Predicción y métricas\n",
        "y_pred = lgbm.predict(X_test)\n",
        "print(\"=== Reporte de clasificación LightGBM ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC AUC (LightGBM):\", roc_auc_score(y_test, lgbm.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# Serializar y descargar el modelo\n",
        "joblib.dump(lgbm, \"lgbm_continuity_model.pkl\")\n",
        "print(\"✅ LightGBM serializado en 'lgbm_continuity_model.pkl'\")\n",
        "\n",
        "gfiles.download(\"lgbm_continuity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "zsMo7g3f8Fhi",
        "outputId": "f6f0a95c-0ccb-4fcb-eab7-42edc04a8265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Reporte de clasificación LightGBM ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9689    0.9333    0.9508       300\n",
            "           1     0.9868    0.9940    0.9904      1502\n",
            "\n",
            "    accuracy                         0.9839      1802\n",
            "   macro avg     0.9778    0.9637    0.9706      1802\n",
            "weighted avg     0.9838    0.9839    0.9838      1802\n",
            "\n",
            "ROC AUC (LightGBM): 0.9982789613848203\n",
            "✅ LightGBM serializado en 'lgbm_continuity_model.pkl'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_75650961-82e5-4e5e-93f9-939d1f5bd50d\", \"lgbm_continuity_model.pkl\", 674548)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Entrenamiento con RF con XGB**"
      ],
      "metadata": {
        "id": "sYb5Ou_sGjQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENTRENAMIENTO DE RF & XGB ===\n",
        "!pip install xgboost\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# División estratificada (X, y, SEED ya definidos en celdas anteriores)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "# 1) RandomForest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "print(\"✅ Random Forest entrenado\")\n",
        "\n",
        "# 2) XGBoost\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.1,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb.fit(X_train, y_train)\n",
        "print(\"✅ XGBoost entrenado\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcvegOZ_9Jhg",
        "outputId": "eeeb9bed-0776-44d2-fea3-017fed34f5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.2.6)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.26.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n",
            "✅ Random Forest entrenado\n",
            "✅ XGBoost entrenado\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [03:08:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.1 Evaluación y generación modelo**"
      ],
      "metadata": {
        "id": "6o61T8gbG67L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUACIÓN Y GUARDADO DE RF y XGB ===\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import joblib\n",
        "from google.colab import files as gfiles\n",
        "\n",
        "# -- Evaluar Random Forest --\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"=== RF Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred_rf, digits=4))\n",
        "print(\"RF ROC AUC:\", roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# -- Evaluar XGBoost --\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "print(\"\\n=== XGB Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred_xgb, digits=4))\n",
        "print(\"XGB ROC AUC:\", roc_auc_score(y_test, xgb.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# -- Guardar modelos --\n",
        "joblib.dump(rf, \"rf_continuity_model.pkl\")\n",
        "joblib.dump(xgb, \"xgb_continuity_model.pkl\")\n",
        "print(\"\\n✅ Modelos serializados: rf_continuity_model.pkl, xgb_continuity_model.pkl\")\n",
        "\n",
        "# -- Descargar --\n",
        "gfiles.download(\"rf_continuity_model.pkl\")\n",
        "gfiles.download(\"xgb_continuity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "GYsYj-Oe905M",
        "outputId": "a13ca2d8-4a7e-4841-a2ba-e29158f24740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RF Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9601    0.9633    0.9617       300\n",
            "           1     0.9927    0.9920    0.9923      1502\n",
            "\n",
            "    accuracy                         0.9872      1802\n",
            "   macro avg     0.9764    0.9777    0.9770      1802\n",
            "weighted avg     0.9873    0.9872    0.9872      1802\n",
            "\n",
            "RF ROC AUC: 0.9963371060807812\n",
            "\n",
            "=== XGB Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9660    0.9467    0.9562       300\n",
            "           1     0.9894    0.9933    0.9914      1502\n",
            "\n",
            "    accuracy                         0.9856      1802\n",
            "   macro avg     0.9777    0.9700    0.9738      1802\n",
            "weighted avg     0.9855    0.9856    0.9855      1802\n",
            "\n",
            "XGB ROC AUC: 0.9984598313359965\n",
            "\n",
            "✅ Modelos serializados: rf_continuity_model.pkl, xgb_continuity_model.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0cbe0689-3f38-4966-8b04-9ed65b8ea0ab\", \"rf_continuity_model.pkl\", 2778889)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8b51e780-019b-4bfc-a9f7-bc5e28abcc4b\", \"xgb_continuity_model.pkl\", 531962)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Entrenamiento con iForest**"
      ],
      "metadata": {
        "id": "pd3XFnOdHDq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENTRENAMIENTO Y EVALUACIÓN DE ISOLATION FOREST ===\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Entrenamos SOLO con datos íntegros (cont_df) y parametrizamos la poda de anomalías, teniendo en cuenta que este algoritmo es no supervisado\n",
        "iso = IsolationForest(\n",
        "    contamination=ANOMALY_RATIO,  # proporción de outliers sintéticos\n",
        "    random_state=SEED\n",
        ")\n",
        "iso.fit(cont_df.values)\n",
        "print(\"✅ Isolation Forest entrenado sobre datos íntegros\")\n",
        "\n",
        "# Predecimos sobre el dataset combinado X\n",
        "# iso.predict:  1 = normal, -1 = anómalo\n",
        "pred_iso = iso.predict(X)\n",
        "y_pred = np.where(pred_iso == 1, 1, 0)\n",
        "\n",
        "# Reporte de métricas\n",
        "print(\"=== Reporte de clasificación IsolationForest ===\")\n",
        "print(classification_report(y, y_pred, digits=4))\n",
        "\n",
        "# Para ROC AUC usamos score de anomalía (más alto = más anómalo)\n",
        "scores = -iso.decision_function(X)\n",
        "print(\"ROC AUC (IsolationForest):\", roc_auc_score(y, scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfs-gVtnClce",
        "outputId": "18025c54-ac0b-45fe-ec4f-19d33edc6301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Isolation Forest entrenado sobre datos íntegros\n",
            "=== Reporte de clasificación IsolationForest ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4987    0.9953    0.6644      1501\n",
            "           1     0.9988    0.8000    0.8884      7509\n",
            "\n",
            "    accuracy                         0.8325      9010\n",
            "   macro avg     0.7488    0.8977    0.7764      9010\n",
            "weighted avg     0.9155    0.8325    0.8511      9010\n",
            "\n",
            "ROC AUC (IsolationForest): 0.007900534903308127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.1 Evaluación y generación modelo**"
      ],
      "metadata": {
        "id": "U-okt_ybHkjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === SERIALIZAR Y DESCARGAR EL MODELO ===\n",
        "import joblib\n",
        "from google.colab import files as gfiles\n",
        "\n",
        "joblib.dump(iso, \"iso_continuity_model.pkl\")\n",
        "print(\"✅ Isolation Forest serializado en 'iso_continuity_model.pkl'\")\n",
        "\n",
        "gfiles.download(\"iso_continuity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ZBwoyN9KCpl-",
        "outputId": "2051231b-574b-4f14-e8d2-8dd986e3c95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Isolation Forest serializado en 'iso_continuity_model.pkl'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b846aa5f-786f-4b95-bc1e-325cabb7e168\", \"iso_continuity_model.pkl\", 1328009)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. Entrenamiento con SVM**"
      ],
      "metadata": {
        "id": "JzItTZp2Hudw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENTRENAMIENTO DEL SVM ===\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# División estratificada (X, y y SEED ya definidos en celdas anteriores)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "# Configuración y entrenamiento del SVM\n",
        "svm = SVC(\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    gamma='scale',\n",
        "    probability=True,     # Para usar predict_proba()\n",
        "    random_state=SEED\n",
        ")\n",
        "svm.fit(X_train, y_train)\n",
        "print(\"✅ SVM entrenado correctamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoIfc8DvDlj-",
        "outputId": "1d6c9b46-d5dc-499d-b42c-55c05d712af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SVM entrenado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **11.1 Evaluación y generación modelo**"
      ],
      "metadata": {
        "id": "kjzVFxTcIB3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUACIÓN Y GUARDADO DEL SVM ===\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import joblib\n",
        "from google.colab import files as gfiles\n",
        "\n",
        "# Predicciones y métricas\n",
        "y_pred = svm.predict(X_test)\n",
        "print(\"=== Reporte de clasificación SVM ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC AUC (SVM):\", roc_auc_score(y_test, svm.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# Serializar y descargar el modelo\n",
        "joblib.dump(svm, \"svm_continuity_model.pkl\")\n",
        "print(\"✅ SVM serializado en 'svm_continuity_model.pkl'\")\n",
        "gfiles.download(\"svm_continuity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "q4fD3ec1DnmD",
        "outputId": "9ba1e200-e92f-433d-a095-02a28aa421bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Reporte de clasificación SVM ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9590    0.9367    0.9477       300\n",
            "           1     0.9874    0.9920    0.9897      1502\n",
            "\n",
            "    accuracy                         0.9828      1802\n",
            "   macro avg     0.9732    0.9643    0.9687      1802\n",
            "weighted avg     0.9827    0.9828    0.9827      1802\n",
            "\n",
            "ROC AUC (SVM): 0.995947625388371\n",
            "✅ SVM serializado en 'svm_continuity_model.pkl'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_362b4043-c05c-4b5e-be26-c8cc5992f5de\", \"svm_continuity_model.pkl\", 16635)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}