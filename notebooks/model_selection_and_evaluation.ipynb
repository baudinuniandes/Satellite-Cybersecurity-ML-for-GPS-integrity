{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "d58AghKA4r49",
        "fGlzpySg4xbr",
        "PDZKOxrh-KCs",
        "jawlqKI15J7u",
        "xCbTYko95YKM",
        "W93I4nlfDrAY",
        "ymvn9xOQD-DJ",
        "7xn_-yOYEuzw",
        "kjvHrw-MFxpq",
        "L0YjqLNcF255",
        "sYb5Ou_sGjQS",
        "pd3XFnOdHDq2",
        "JzItTZp2Hudw"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Notebook para selecci√≥n del mejor algoritmo de Machine Learning**\n",
        "Con la finalidad de encontrar el algoritmo que mas se ajusta al set de datos propuesto y con el esquema de preselecci√≥n de 6 algortimos que cumpl√≠an con unas caracter√≠sticas especificas, se seleccionaron estos algoritmos para realizar las pruebas:\n",
        "* Multi-Layer Perceptron (MLP)\n",
        "* Random Forest (RF)\n",
        "* Light Gradient Boosting Machine (LightGBM)\n",
        "* Extreme Gradient Boosting (XGB)\n",
        "* Support Vector Machine (SVM)\n",
        "* Isolation Forest (iForest)\n",
        "\n",
        "Se realizaron los ajustes de las librer√≠as necesarias y se efectu√≥ un entrenamiento asistido con los m√©todos ofrecidos por las librer√≠as de cada algoritmo.\n",
        "\n",
        "### **Estructura del notebook:**\n",
        "1. Instalaci√≥n de dependencias\n",
        "2. Cargue de archivos CSV (gps_valid.csv)\n",
        "3. Importe y carga de datos\n",
        "4. Funci√≥n Haversine\n",
        "5. Funci√≥n de generaci√≥n datos spoofing\n",
        "6. Entrenamiento con MLP\n",
        "7. Entrenamiento con RF\n",
        "8. Entrenamiento con LightGBM\n",
        "9. Entrenamiento con RF con XGB\n",
        "10. Entrenamiento con iForest\n",
        "11. Entrenamiento con SVM\n",
        "\n"
      ],
      "metadata": {
        "id": "poZ_Y7IB1DSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Instalaci√≥n de dependencias**"
      ],
      "metadata": {
        "id": "d58AghKA4r49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === INSTALAR DEPENDENCIAS ===\n",
        "!pip install --upgrade --force-reinstall numpy scipy scikit-learn pandas joblib xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J8yxDYWk1TR2",
        "outputId": "192b4868-667e-4d33-94b0-6db49ecd20a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting joblib\n",
            "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting xgboost\n",
            "  Downloading xgboost-3.0.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.26.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xgboost-3.0.1-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (318.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m318.1/318.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, threadpoolctl, six, nvidia-nccl-cu12, numpy, joblib, scipy, python-dateutil, xgboost, scikit-learn, pandas\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.6.0\n",
            "    Uninstalling threadpoolctl-3.6.0:\n",
            "      Successfully uninstalled threadpoolctl-3.6.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.6\n",
            "    Uninstalling numpy-2.2.6:\n",
            "      Successfully uninstalled numpy-2.2.6\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.5.0\n",
            "    Uninstalling joblib-1.5.0:\n",
            "      Successfully uninstalled joblib-1.5.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 2.1.4\n",
            "    Uninstalling xgboost-2.1.4:\n",
            "      Successfully uninstalled xgboost-2.1.4\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.3\n",
            "    Uninstalling pandas-2.2.3:\n",
            "      Successfully uninstalled pandas-2.2.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nccl-cu12==2.21.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nccl-cu12 2.26.5 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.5.0 numpy-2.2.6 nvidia-nccl-cu12-2.26.5 pandas-2.2.3 python-dateutil-2.9.0.post0 pytz-2025.2 scikit-learn-1.6.1 scipy-1.15.3 six-1.17.0 threadpoolctl-3.6.0 tzdata-2025.2 xgboost-3.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "six"
                ]
              },
              "id": "0a4345ea8958461d80af29c04cf36883"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Cargue de archivos CSV (gps_valid.csv)**"
      ],
      "metadata": {
        "id": "fGlzpySg4xbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === SUBIR CSV ===\n",
        "from google.colab import files\n",
        "\n",
        "print(\"üîÉ Sube aqu√≠ tu archivo gps_valid.csv (datos √≠ntegros sin spoofing)\")\n",
        "uploaded = files.upload()  # Selecciona gps_valid.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "tEwdZ-Jy5CFj",
        "outputId": "e0a236e4-8e15-4390-9844-26dc9785b6b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÉ Sube aqu√≠ tu archivo gps_valid.csv (datos √≠ntegros sin spoofing)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cd2b26b8-9f01-4f82-9b79-d00aee0266dd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cd2b26b8-9f01-4f82-9b79-d00aee0266dd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving gps_valid.csv to gps_valid.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Importe y carga de datos**"
      ],
      "metadata": {
        "id": "PDZKOxrh-KCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === IMPORTS Y CARGA DE DATOS ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from math import radians, sin, cos, sqrt, atan2, degrees\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from google.colab import files as gfiles\n",
        "\n",
        "# Nombre del CSV subido\n",
        "CSV_FILE = \"gps_valid.csv\"\n",
        "\n",
        "# Cargar y parsear\n",
        "df = pd.read_csv(CSV_FILE, parse_dates=[\"UTC\"])\n",
        "# Separar Lat/Lon\n",
        "df[[\"Lat\",\"Lon\"]] = df[\"Position\"].str.split(\",\", expand=True).astype(float)\n",
        "df = df.sort_values(\"UTC\").reset_index(drop=True)\n",
        "\n",
        "print(f\"‚úÖ Datos cargados: {len(df)} filas\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQPBCTpi5Q03",
        "outputId": "34a1fb08-bdf7-40a2-cff1-eed00567b092"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Datos cargados: 7510 filas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Funci√≥n Haversine**"
      ],
      "metadata": {
        "id": "jawlqKI15J7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === C√ÅLCULO DE FEATURES DE CONTINUIDAD ===\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371000  # radio terrestre en metros\n",
        "    œÜ1, œÜ2 = radians(lat1), radians(lat2)\n",
        "    ŒîœÜ = radians(lat2 - lat1)\n",
        "    ŒîŒª = radians(lon2 - lon1)\n",
        "    a = sin(ŒîœÜ/2)**2 + cos(œÜ1)*cos(œÜ2)*sin(ŒîŒª/2)**2\n",
        "    d = 2 * R * atan2(sqrt(a), sqrt(1-a))\n",
        "    y = sin(ŒîŒª) * cos(œÜ2)\n",
        "    x = cos(œÜ1)*sin(œÜ2) - sin(œÜ1)*cos(œÜ2)*cos(ŒîŒª)\n",
        "    Œ∏ = (degrees(atan2(y, x)) + 360) % 360\n",
        "    return d, Œ∏\n",
        "\n",
        "def compute_continuity_features(df):\n",
        "    dists, bears = [], []\n",
        "    for i in range(1, len(df)):\n",
        "        d, b = haversine(\n",
        "            df.loc[i-1,\"Lat\"], df.loc[i-1,\"Lon\"],\n",
        "            df.loc[i  ,\"Lat\"], df.loc[i  ,\"Lon\"]\n",
        "        )\n",
        "        dists.append(d)\n",
        "        bears.append(b)\n",
        "    return pd.DataFrame({\n",
        "        \"dist\":    np.array(dists, dtype=np.float32),\n",
        "        \"bearing\": np.array(bears, dtype=np.float32)\n",
        "    })\n",
        "\n",
        "cont_df = compute_continuity_features(df)\n",
        "y_clean = np.ones(len(cont_df), dtype=int)\n",
        "\n",
        "print(f\"‚úÖ {len(cont_df)} muestras de continuidad calculadas\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSl82JSJ5WKl",
        "outputId": "2204188a-d3f5-4d75-dafb-d72091f7b3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ 7509 muestras de continuidad calculadas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Funci√≥n de generaci√≥n datos spoofing**"
      ],
      "metadata": {
        "id": "xCbTYko95YKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === GENERACI√ìN DE ANOMAL√çAS SINT√âTICAS ===\n",
        "SEED = 42\n",
        "ANOMALY_RATIO = 0.2\n",
        "\n",
        "np.random.seed(SEED)\n",
        "n = len(cont_df)\n",
        "n_anom = int(n * ANOMALY_RATIO)\n",
        "# Seleccion de indices del dataset original\n",
        "idx = np.random.choice(n, n_anom, replace=False)\n",
        "\n",
        "# Creacion de un arreglo con valores de spoofing generados\n",
        "cont_anom_features = []\n",
        "for j in idx:\n",
        "    # Toma de datos del dataset original\n",
        "    d_orig, b_orig = cont_df.loc[j, \"dist\"], cont_df.loc[j, \"bearing\"]\n",
        "    lat0, lon0 = df.loc[j, \"Lat\"], df.loc[j, \"Lon\"] # Uso de la funcion Haversine para carcualar valores de latitud y longitud\n",
        "\n",
        "    # Generacion de parametros aleatorios fuera de ruta entre 5 y 50 kilometros (teniendo en cuenta que el radio esta en kilometros)\n",
        "    jump_d = np.random.uniform(5_000, 50_000)\n",
        "    jump_b = np.random.uniform(0, 360)\n",
        "\n",
        "    # El calculo asume que se tiene un salto a una nueva posicion, simulando un spoofing\n",
        "    # La anomalia estaria en el punto generado (df.loc[j])\n",
        "    œÜ = radians(lat0)\n",
        "    ŒîœÜ = (jump_d * cos(radians(jump_b))) / 6371000\n",
        "    ŒîŒª = (jump_d * sin(radians(jump_b))) / (6371000 * cos(œÜ))\n",
        "    lat1 = lat0 + degrees(ŒîœÜ)\n",
        "    lon1 = lon0 + degrees(ŒîŒª)\n",
        "\n",
        "    '''\n",
        "    Calcular las caracter√≠sticas con la funcion  Haversine para el punto an√≥malo en relaci√≥n con el punto anterior, asumiendo que el punto anterior es el que est√° en el √≠ndice j-1 en el DataFrame original (df).\n",
        "    Se usara la interpretaci√≥n de que el salto modifica la ubicaci√≥n del punto j y que la anomal√≠a se detecta en la caracter√≠stica de continuidad entre j-1 y el j modificado.\n",
        "    Esto significa que el c√°lculo con la funcion Haversine debe realizarse entre `df.loc[j-1]` y la nueva ubicaci√≥n `(lat1, lon1)`.\n",
        "    Las caracter√≠sticas `dist` y `bearing` en el √≠ndice `i` dentro de `cont_df` corresponden a la continuidad entre `df.loc[i-1]` y `df.loc[i]`.\n",
        "    Si queremos introducir una anomal√≠a en el √≠ndice `j` de `cont_df`, deber√≠amos modificar las caracter√≠sticas de continuidad entre `df.loc[j-1]` y una nueva ubicaci√≥n para `df.loc[j]`.\n",
        "\n",
        "    Las caracter√≠sticas an√≥malas deber√≠an calcularse entre `df.loc[j]` y una nueva ubicaci√≥n para `df.loc[j+1]`.\n",
        "    El c√≥digo original usaba `df.loc[j]` como punto de inicio del salto y aplicaba el salto para obtener un nuevo punto (lat1, lon1).\n",
        "    Luego calculaba Haversine entre `df.loc[j]` y este nuevo punto. Este c√°lculo no reemplaza directamente el valor original en `cont_df[j]` (que es entre `df[j]` y `df[j+1]`).\n",
        "\n",
        "    Supongamos entonces que el objetivo es crear puntos de spoofing. Un paso an√≥malo en el √≠ndice `j` de `cont_df` significa que la transici√≥n desde `df.loc[j]` hacia `df.loc[j+1]` es an√≥mala.\n",
        "    Deber√≠amos tomar `df.loc[j]` como punto de partida y aplicar un salto para encontrar una nueva ubicaci√≥n para el siguiente punto (que ser√≠a el punto an√≥malo `df.loc[j+1]`).\n",
        "    '''\n",
        "\n",
        "    lat_start, lon_start = df.loc[j, \"Lat\"], df.loc[j, \"Lon\"]\n",
        "\n",
        "    œÜ_start = radians(lat_start)\n",
        "    ŒîœÜ = (jump_d * cos(radians(jump_b))) / 6371000\n",
        "    ŒîŒª = (jump_d * sin(radians(jump_b))) / (6371000 * cos(œÜ_start))\n",
        "    lat_anom_next = lat_start + degrees(ŒîœÜ)\n",
        "    lon_anom_next = lon_start + degrees(ŒîŒª)\n",
        "\n",
        "    # Calcular las caracter√≠sticas de Haversine para este paso an√≥malo (entre df.loc[j] y la nueva ubicaci√≥n para df.loc[j+1])\n",
        "    d_anom, b_anom = haversine(lat_start, lon_start, lat_anom_next, lon_anom_next)\n",
        "    cont_anom_features.append([d_anom, b_anom])\n",
        "\n",
        "# Se convierte el listado de anomalias en un numpy array\n",
        "cont_anom_features = np.array(cont_anom_features, dtype=np.float32)\n",
        "\n",
        "y_anom = np.zeros(n_anom, dtype=int)\n",
        "\n",
        "# Se combinan los datos del dataset entre integros 1 y anomalos o spoofing 0\n",
        "X = np.vstack([cont_df.values, cont_anom_features])\n",
        "y = np.concatenate([y_clean, y_anom])\n",
        "\n",
        "print(f\"‚úÖ Dataset combinado: {X.shape[0]} muestras ({len(y_clean)} √≠ntegros + {n_anom} an√≥malos)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ie6LdTxF5b20",
        "outputId": "2e6146bf-5c55-45d5-a5bd-7945cd33620e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset combinado: 9010 muestras (7509 √≠ntegros + 1501 an√≥malos)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Entrenamiento con MLP**"
      ],
      "metadata": {
        "id": "W93I4nlfDrAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENTRENAMIENTO MLP ===\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(64, 32),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=200,\n",
        "    random_state=SEED,\n",
        "    verbose=True\n",
        ")\n",
        "mlp.fit(X_train, y_train)\n",
        "print(\"‚úÖ MLP entrenado correctamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuufZEwn5kcK",
        "outputId": "f75a8497-3b15-4bc4-ca0a-b23e632990f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 5.57524007\n",
            "Iteration 2, loss = 2.32617669\n",
            "Iteration 3, loss = 1.64645100\n",
            "Iteration 4, loss = 2.05241832\n",
            "Iteration 5, loss = 1.96156036\n",
            "Iteration 6, loss = 2.08208372\n",
            "Iteration 7, loss = 1.88712903\n",
            "Iteration 8, loss = 1.53279043\n",
            "Iteration 9, loss = 1.47457927\n",
            "Iteration 10, loss = 1.77926970\n",
            "Iteration 11, loss = 1.96757365\n",
            "Iteration 12, loss = 1.93428265\n",
            "Iteration 13, loss = 1.69848470\n",
            "Iteration 14, loss = 1.23135989\n",
            "Iteration 15, loss = 1.70129981\n",
            "Iteration 16, loss = 2.41347051\n",
            "Iteration 17, loss = 2.60123147\n",
            "Iteration 18, loss = 1.73810149\n",
            "Iteration 19, loss = 2.97231880\n",
            "Iteration 20, loss = 1.59230060\n",
            "Iteration 21, loss = 1.38349140\n",
            "Iteration 22, loss = 2.17824599\n",
            "Iteration 23, loss = 1.94167175\n",
            "Iteration 24, loss = 1.87374947\n",
            "Iteration 25, loss = 2.41018855\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "‚úÖ MLP entrenado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.1 Evaluaci√≥n y generaci√≥n modelo**"
      ],
      "metadata": {
        "id": "ymvn9xOQD-DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUACI√ìN Y GUARDADO DEL MLP ===\n",
        "y_pred = mlp.predict(X_test)\n",
        "print(\"=== Reporte de clasificaci√≥n MLP ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC AUC (MLP):\", roc_auc_score(y_test, mlp.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# Serializar y descargar el modelo\n",
        "joblib.dump(mlp, \"mlp_continuity_model.pkl\")\n",
        "print(\"‚úÖ MLP serializado en 'mlp_continuity_model.pkl'\")\n",
        "gfiles.download(\"mlp_continuity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "2Tmv07jO54Uo",
        "outputId": "3a5643fa-c0ea-43ee-9552-04a3b13fd953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Reporte de clasificaci√≥n MLP ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6489    0.8133    0.7219       300\n",
            "           1     0.9607    0.9121    0.9358      1502\n",
            "\n",
            "    accuracy                         0.8957      1802\n",
            "   macro avg     0.8048    0.8627    0.8288      1802\n",
            "weighted avg     0.9088    0.8957    0.9002      1802\n",
            "\n",
            "ROC AUC (MLP): 0.9358710608078118\n",
            "‚úÖ MLP serializado en 'mlp_continuity_model.pkl'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_02039291-6428-4b4c-921e-4fe46e436c6f\", \"mlp_continuity_model.pkl\", 45553)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Entrenamiento con RF**"
      ],
      "metadata": {
        "id": "7xn_-yOYEuzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENTRENAMIENTO DEL RANDOM FOREST ===\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Divisi√≥n estratificada (usa X, y ya definidos en celdas anteriores)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "# Configuraci√≥n y entrenamiento del Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "print(\"‚úÖ Random Forest entrenado correctamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33xbIOMR7hOO",
        "outputId": "be3868f5-b4b2-4101-dadf-f8dfde2e4b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Random Forest entrenado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.1 Evaluaci√≥n y generaci√≥n modelo**"
      ],
      "metadata": {
        "id": "kjvHrw-MFxpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUACI√ìN Y GUARDADO DEL RANDOM FOREST ===\n",
        "# Predicciones y m√©tricas\n",
        "y_pred = rf.predict(X_test)\n",
        "print(\"=== Reporte de clasificaci√≥n RF ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC AUC (RF):\", roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# Serializar y descargar el modelo\n",
        "joblib.dump(rf, \"rf_continuity_model.pkl\")\n",
        "print(\"‚úÖ RF serializado en 'rf_continuity_model.pkl'\")\n",
        "gfiles.download(\"rf_continuity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "cSgMqJq77joF",
        "outputId": "56b6c58e-f293-4a63-afa4-e473e2561c79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Reporte de clasificaci√≥n RF ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9601    0.9633    0.9617       300\n",
            "           1     0.9927    0.9920    0.9923      1502\n",
            "\n",
            "    accuracy                         0.9872      1802\n",
            "   macro avg     0.9764    0.9777    0.9770      1802\n",
            "weighted avg     0.9873    0.9872    0.9872      1802\n",
            "\n",
            "ROC AUC (RF): 0.9963371060807812\n",
            "‚úÖ RF serializado en 'rf_continuity_model.pkl'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_63713a68-fa09-4903-94ea-1da5907f529e\", \"rf_continuity_model.pkl\", 2778889)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Entrenamiento con LightGBM**"
      ],
      "metadata": {
        "id": "L0YjqLNcF255"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENTRENAR con LightGBM ===\n",
        "!pip install lightgbm\n",
        "\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Usamos X, y, SEED ya definidos en las celdas anteriores\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "# Configuraci√≥n y entrenamiento del modelo LightGBM\n",
        "lgbm = LGBMClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.1,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "lgbm.fit(X_train, y_train)\n",
        "print(\"‚úÖ LightGBM entrenado correctamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wTckVTK7rSL",
        "outputId": "834804da-248e-4657-9201-72c12f0113a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
            "[LightGBM] [Info] Number of positive: 6007, number of negative: 1201\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000783 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 510\n",
            "[LightGBM] [Info] Number of data points in the train set: 7208, number of used features: 2\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.833380 -> initscore=1.609771\n",
            "[LightGBM] [Info] Start training from score 1.609771\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "‚úÖ LightGBM entrenado correctamente\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8.1 Evaluaci√≥n y generaci√≥n modelo**"
      ],
      "metadata": {
        "id": "LYwprR5_GMsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUACI√ìN Y GUARDADO LightGBM ===\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import joblib\n",
        "from google.colab import files as gfiles\n",
        "\n",
        "# Predicci√≥n y m√©tricas\n",
        "y_pred = lgbm.predict(X_test)\n",
        "print(\"=== Reporte de clasificaci√≥n LightGBM ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC AUC (LightGBM):\", roc_auc_score(y_test, lgbm.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# Serializar y descargar el modelo\n",
        "joblib.dump(lgbm, \"lgbm_continuity_model.pkl\")\n",
        "print(\"‚úÖ LightGBM serializado en 'lgbm_continuity_model.pkl'\")\n",
        "\n",
        "gfiles.download(\"lgbm_continuity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "zsMo7g3f8Fhi",
        "outputId": "f6f0a95c-0ccb-4fcb-eab7-42edc04a8265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Reporte de clasificaci√≥n LightGBM ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9689    0.9333    0.9508       300\n",
            "           1     0.9868    0.9940    0.9904      1502\n",
            "\n",
            "    accuracy                         0.9839      1802\n",
            "   macro avg     0.9778    0.9637    0.9706      1802\n",
            "weighted avg     0.9838    0.9839    0.9838      1802\n",
            "\n",
            "ROC AUC (LightGBM): 0.9982789613848203\n",
            "‚úÖ LightGBM serializado en 'lgbm_continuity_model.pkl'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_75650961-82e5-4e5e-93f9-939d1f5bd50d\", \"lgbm_continuity_model.pkl\", 674548)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Entrenamiento con RF con XGB**"
      ],
      "metadata": {
        "id": "sYb5Ou_sGjQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENTRENAMIENTO DE RF & XGB ===\n",
        "!pip install xgboost\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Divisi√≥n estratificada (X, y, SEED ya definidos en celdas anteriores)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "# 1) RandomForest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "print(\"‚úÖ Random Forest entrenado\")\n",
        "\n",
        "# 2) XGBoost\n",
        "xgb = XGBClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=10,\n",
        "    learning_rate=0.1,\n",
        "    use_label_encoder=False,\n",
        "    eval_metric=\"logloss\",\n",
        "    random_state=SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "xgb.fit(X_train, y_train)\n",
        "print(\"‚úÖ XGBoost entrenado\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcvegOZ_9Jhg",
        "outputId": "eeeb9bed-0776-44d2-fea3-017fed34f5ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.2.6)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.26.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.3)\n",
            "‚úÖ Random Forest entrenado\n",
            "‚úÖ XGBoost entrenado\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [03:08:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9.1 Evaluaci√≥n y generaci√≥n modelo**"
      ],
      "metadata": {
        "id": "6o61T8gbG67L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUACI√ìN Y GUARDADO DE RF y XGB ===\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import joblib\n",
        "from google.colab import files as gfiles\n",
        "\n",
        "# -- Evaluar Random Forest --\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "print(\"=== RF Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred_rf, digits=4))\n",
        "print(\"RF ROC AUC:\", roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# -- Evaluar XGBoost --\n",
        "y_pred_xgb = xgb.predict(X_test)\n",
        "print(\"\\n=== XGB Classification Report ===\")\n",
        "print(classification_report(y_test, y_pred_xgb, digits=4))\n",
        "print(\"XGB ROC AUC:\", roc_auc_score(y_test, xgb.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# -- Guardar modelos --\n",
        "joblib.dump(rf, \"rf_continuity_model.pkl\")\n",
        "joblib.dump(xgb, \"xgb_continuity_model.pkl\")\n",
        "print(\"\\n‚úÖ Modelos serializados: rf_continuity_model.pkl, xgb_continuity_model.pkl\")\n",
        "\n",
        "# -- Descargar --\n",
        "gfiles.download(\"rf_continuity_model.pkl\")\n",
        "gfiles.download(\"xgb_continuity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "GYsYj-Oe905M",
        "outputId": "a13ca2d8-4a7e-4841-a2ba-e29158f24740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== RF Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9601    0.9633    0.9617       300\n",
            "           1     0.9927    0.9920    0.9923      1502\n",
            "\n",
            "    accuracy                         0.9872      1802\n",
            "   macro avg     0.9764    0.9777    0.9770      1802\n",
            "weighted avg     0.9873    0.9872    0.9872      1802\n",
            "\n",
            "RF ROC AUC: 0.9963371060807812\n",
            "\n",
            "=== XGB Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9660    0.9467    0.9562       300\n",
            "           1     0.9894    0.9933    0.9914      1502\n",
            "\n",
            "    accuracy                         0.9856      1802\n",
            "   macro avg     0.9777    0.9700    0.9738      1802\n",
            "weighted avg     0.9855    0.9856    0.9855      1802\n",
            "\n",
            "XGB ROC AUC: 0.9984598313359965\n",
            "\n",
            "‚úÖ Modelos serializados: rf_continuity_model.pkl, xgb_continuity_model.pkl\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0cbe0689-3f38-4966-8b04-9ed65b8ea0ab\", \"rf_continuity_model.pkl\", 2778889)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8b51e780-019b-4bfc-a9f7-bc5e28abcc4b\", \"xgb_continuity_model.pkl\", 531962)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **10. Entrenamiento con iForest**"
      ],
      "metadata": {
        "id": "pd3XFnOdHDq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENTRENAMIENTO Y EVALUACI√ìN DE ISOLATION FOREST ===\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Entrenamos SOLO con datos √≠ntegros (cont_df) y parametrizamos la poda de anomal√≠as, teniendo en cuenta que este algoritmo es no supervisado\n",
        "iso = IsolationForest(\n",
        "    contamination=ANOMALY_RATIO,  # proporci√≥n de outliers sint√©ticos\n",
        "    random_state=SEED\n",
        ")\n",
        "iso.fit(cont_df.values)\n",
        "print(\"‚úÖ Isolation Forest entrenado sobre datos √≠ntegros\")\n",
        "\n",
        "# Predecimos sobre el dataset combinado X\n",
        "# iso.predict:  1 = normal, -1 = an√≥malo\n",
        "pred_iso = iso.predict(X)\n",
        "y_pred = np.where(pred_iso == 1, 1, 0)\n",
        "\n",
        "# Reporte de m√©tricas\n",
        "print(\"=== Reporte de clasificaci√≥n IsolationForest ===\")\n",
        "print(classification_report(y, y_pred, digits=4))\n",
        "\n",
        "# Para ROC AUC usamos score de anomal√≠a (m√°s alto = m√°s an√≥malo)\n",
        "scores = -iso.decision_function(X)\n",
        "print(\"ROC AUC (IsolationForest):\", roc_auc_score(y, scores))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfs-gVtnClce",
        "outputId": "18025c54-ac0b-45fe-ec4f-19d33edc6301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Isolation Forest entrenado sobre datos √≠ntegros\n",
            "=== Reporte de clasificaci√≥n IsolationForest ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4987    0.9953    0.6644      1501\n",
            "           1     0.9988    0.8000    0.8884      7509\n",
            "\n",
            "    accuracy                         0.8325      9010\n",
            "   macro avg     0.7488    0.8977    0.7764      9010\n",
            "weighted avg     0.9155    0.8325    0.8511      9010\n",
            "\n",
            "ROC AUC (IsolationForest): 0.007900534903308127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10.1 Evaluaci√≥n y generaci√≥n modelo**"
      ],
      "metadata": {
        "id": "U-okt_ybHkjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === SERIALIZAR Y DESCARGAR EL MODELO ===\n",
        "import joblib\n",
        "from google.colab import files as gfiles\n",
        "\n",
        "joblib.dump(iso, \"iso_continuity_model.pkl\")\n",
        "print(\"‚úÖ Isolation Forest serializado en 'iso_continuity_model.pkl'\")\n",
        "\n",
        "gfiles.download(\"iso_continuity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ZBwoyN9KCpl-",
        "outputId": "2051231b-574b-4f14-e8d2-8dd986e3c95a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Isolation Forest serializado en 'iso_continuity_model.pkl'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b846aa5f-786f-4b95-bc1e-325cabb7e168\", \"iso_continuity_model.pkl\", 1328009)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **11. Entrenamiento con SVM**"
      ],
      "metadata": {
        "id": "JzItTZp2Hudw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ENTRENAMIENTO DEL SVM ===\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Divisi√≥n estratificada (X, y y SEED ya definidos en celdas anteriores)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "# Configuraci√≥n y entrenamiento del SVM\n",
        "svm = SVC(\n",
        "    kernel='rbf',\n",
        "    C=1.0,\n",
        "    gamma='scale',\n",
        "    probability=True,     # Para usar predict_proba()\n",
        "    random_state=SEED\n",
        ")\n",
        "svm.fit(X_train, y_train)\n",
        "print(\"‚úÖ SVM entrenado correctamente\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoIfc8DvDlj-",
        "outputId": "1d6c9b46-d5dc-499d-b42c-55c05d712af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ SVM entrenado correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **11.1 Evaluaci√≥n y generaci√≥n modelo**"
      ],
      "metadata": {
        "id": "kjzVFxTcIB3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUACI√ìN Y GUARDADO DEL SVM ===\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "import joblib\n",
        "from google.colab import files as gfiles\n",
        "\n",
        "# Predicciones y m√©tricas\n",
        "y_pred = svm.predict(X_test)\n",
        "print(\"=== Reporte de clasificaci√≥n SVM ===\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ROC AUC (SVM):\", roc_auc_score(y_test, svm.predict_proba(X_test)[:,1]))\n",
        "\n",
        "# Serializar y descargar el modelo\n",
        "joblib.dump(svm, \"svm_continuity_model.pkl\")\n",
        "print(\"‚úÖ SVM serializado en 'svm_continuity_model.pkl'\")\n",
        "gfiles.download(\"svm_continuity_model.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "q4fD3ec1DnmD",
        "outputId": "9ba1e200-e92f-433d-a095-02a28aa421bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Reporte de clasificaci√≥n SVM ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9590    0.9367    0.9477       300\n",
            "           1     0.9874    0.9920    0.9897      1502\n",
            "\n",
            "    accuracy                         0.9828      1802\n",
            "   macro avg     0.9732    0.9643    0.9687      1802\n",
            "weighted avg     0.9827    0.9828    0.9827      1802\n",
            "\n",
            "ROC AUC (SVM): 0.995947625388371\n",
            "‚úÖ SVM serializado en 'svm_continuity_model.pkl'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_362b4043-c05c-4b5e-be26-c8cc5992f5de\", \"svm_continuity_model.pkl\", 16635)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}